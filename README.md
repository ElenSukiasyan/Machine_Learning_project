# Machine_Learning_project

## Participants:
Alisa Mkrtchyan, Diana Tumasyan, Elen Sukiasyan

### Naive Bayes: 
A simple probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features. It is commonly used for text classification and spam filtering.
### K-Nearest Neighbors (KNN): 
A non-parametric classification algorithm that assigns an instance to the majority class among its k nearest neighbors in the feature space. It's used for classification and regression tasks.
### K-Means: 
A clustering algorithm that partitions data into k clusters by iteratively assigning each data point to the cluster with the nearest mean, and then recalculating the mean of each cluster. It's widely used for clustering unlabeled data. 
### K-Medoids: 
A variant of K-Means clustering that uses medoids (the most centrally located point in a cluster) instead of means. It's more robust to outliers than K-Means. Decision Tree: A supervised learning algorithm that recursively splits the data into subsets based on the feature that best separates the classes. It's interpretable and can handle both categorical and numerical data.
### DBSCAN (Density-Based Spatial Clustering of Applications with Noise):
A density-based clustering algorithm that groups together points that are closely packed together, marking outliers as noise. It's suitable for data with complex shapes and varying densities. 
### Bagging (Bootstrap Aggregating):
An ensemble method that builds multiple models (typically decision trees) independently and then combines their predictions through averaging or voting to reduce overfitting and improve accuracy. 
### Gaussian Mixture Model (GMM): 
A probabilistic model that represents the distribution of data as a mixture of several Gaussian distributions. It's commonly used for density estimation and clustering.
### Principal Component Analysis (PCA): 
A dimensionality reduction technique that transforms the original features into a lower-dimensional space while preserving most of the variance in the data. It's useful for visualization and speeding up subsequent learning algorithms. In our project, we employed these algorithms to perform various tasks such as classification, clustering, and dimensionality reduction, depending on the specific requirements and nature of the data. Each algorithm was chosen based on its suitability for the task at hand and its performance in preliminary evaluations.

Also we used several visualization (can be find in visualizations.py file), that shows some interesting relationships in our data.
